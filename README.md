The Wikipedia data harvest project revolves around the extraction of data from Wikipedia, specifically focusing on universities, and its systematic integration into various databases. The primary objective is to automate the retrieval and storage of university-related information for diverse database platforms, initially encompassing MySQL, Postgres, and SQL Server.

Business Overview/Problem
In the ever-evolving landscape of educational data management, the absence of an automated and versatile solution for acquiring comprehensive university information presents a significant challenge. Manually collecting and updating data from Wikipedia is time-consuming, error-prone, and lacks scalability. The need for a streamlined process to systematically harvest university data from Wikipedia and integrate it into various databases has become increasingly apparent.

Rationale for the Project
The Wikipedia data harvest project revolves around the extraction of data from Wikipedia, specifically focusing on universities, and its systematic integration into various databases. The primary objective is to automate the retrieval and storage of university-related information for diverse database platforms, initially encompassing MySQL, Postgres, and SQL Server.

Aim of the Project
A. Implement Web Scraping: Develop a robust web scraping module using Python libraries to extract relevant university data from Wikipedia pages

 

B. Database Setup and Configuration: Set up and configure databases with Docker to establish a foundation for data storage

 

C. Database Schema Design: Design and implement appropriate database schemas to accommodate the scraped university data

 

D. Database Integration: Develop scripts to load scraped data seamlessly into the designated databases

 

E. SQL Query Development: Craft SQL queries for creating, reading, updating, and deleting data in the databases

 

F. Cost Efficiency: Optimize resource utilization and costs by leveraging Distributed SQL Queries.

